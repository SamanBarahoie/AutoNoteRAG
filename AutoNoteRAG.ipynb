{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16c767727f5747e9b65d70d9d342b04a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05b263dbaf9b4c9cb4b26348f889fddf",
              "IPY_MODEL_b2493d2220db471992282084ab05e355",
              "IPY_MODEL_ce7a8faeaea74f7bbd44e5662b864d73"
            ],
            "layout": "IPY_MODEL_7db6e13d1b9e4435aee706b131533b62"
          }
        },
        "05b263dbaf9b4c9cb4b26348f889fddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bf3166a218e4f8aada27ec452730ddc",
            "placeholder": "​",
            "style": "IPY_MODEL_b75ff2638dc84f6cb9b5acf4c410027f",
            "value": "Batches: 100%"
          }
        },
        "b2493d2220db471992282084ab05e355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f281b464eb674df9acc2636bea9141c5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05d65b9190a74b169d0486ae886246fb",
            "value": 1
          }
        },
        "ce7a8faeaea74f7bbd44e5662b864d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d109555a90b4a189e32f09f327e26c1",
            "placeholder": "​",
            "style": "IPY_MODEL_ad1b12e395624110b33fa3f0a80d388f",
            "value": " 1/1 [00:00&lt;00:00,  1.27it/s]"
          }
        },
        "7db6e13d1b9e4435aee706b131533b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf3166a218e4f8aada27ec452730ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b75ff2638dc84f6cb9b5acf4c410027f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f281b464eb674df9acc2636bea9141c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d65b9190a74b169d0486ae886246fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d109555a90b4a189e32f09f327e26c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad1b12e395624110b33fa3f0a80d388f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "tbQMy8WGVRow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Meeting Summarizer — Colab-ready Python script\n",
        "# Run in Google Colab (or any Python env).\n",
        "# Requirements: sentence-transformers, faiss-cpu, requests\n",
        "\n",
        "# ----------------------\n",
        "# 1) Install dependencies (run this cell in Colab)\n",
        "# ----------------------\n",
        "!pip install -q sentence-transformers faiss-cpu requests\n"
      ],
      "metadata": {
        "id": "wc4p-vtf6Db_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "GoVgjj8pVYhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import math\n",
        "import requests\n",
        "from typing import List, Tuple\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n"
      ],
      "metadata": {
        "id": "eQ3pi6sx6EkH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "uwOo5u88W1yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "API_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self, model: str, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = API_BASE_URL\n",
        "        self.model = model\n",
        "\n",
        "    def generate_response(self, prompt: str, system:str, temperature: float = 0.2, max_tokens: int = 512) -> str:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        data = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "        response = requests.post(f\"{self.base_url}/chat/completions\", headers=headers, json=data)\n",
        "        response.raise_for_status()\n",
        "        json = response.json()\n",
        "\n",
        "        # API returns a structure as json\n",
        "        return json[\"choices\"][0][\"message\"][\"content\"].strip()\n"
      ],
      "metadata": {
        "id": "-_NcmNKV6Hp3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "WtlLG6pKW-KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "\n",
        "    # Basic cleaning — feel free to extend (remove timestamps, filler words etc.)\n",
        "    text = text.replace('\\r', '\\n')\n",
        "    # normalize extra spaces\n",
        "    text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def chunk_text(text: str, max_chars: int = 1000) -> List[str]:\n",
        "    # Naive chunking by characters while preserving line breaks and speaker turns\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    chunks = []\n",
        "    cur = []\n",
        "    cur_len = 0\n",
        "\n",
        "    for ln in lines:\n",
        "        if cur_len + len(ln) + 1 > max_chars and cur:\n",
        "            chunks.append(\"\\n\".join(cur))\n",
        "            cur = [ln]\n",
        "            cur_len = len(ln)\n",
        "        else:\n",
        "            cur.append(ln)\n",
        "            cur_len += len(ln) + 1\n",
        "    if cur:\n",
        "        chunks.append(\"\\n\".join(cur))\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "zdUiGtL16Ja3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build embeddings"
      ],
      "metadata": {
        "id": "juomiZtmXSa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_embeddings_index(chunks: List[str], embed_model_name: str = 'all-MiniLM-L6-v2') -> Tuple[SentenceTransformer, np.ndarray, faiss.IndexFlatL2]:\n",
        "    model = SentenceTransformer(embed_model_name)\n",
        "    embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return model, embeddings, index\n",
        "\n"
      ],
      "metadata": {
        "id": "hfdSKFoB6LXn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "fe1upMNCXXKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def retrieve(query: str, embed_model: SentenceTransformer, index: faiss.IndexFlatL2, chunks: List[str], k: int = 4) -> List[Tuple[int, float, str]]:\n",
        "\n",
        "    qv = embed_model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(qv, k)\n",
        "    results = []\n",
        "\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        results.append((int(idx), float(score), chunks[idx]))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "8BRGwjr-6O7n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt"
      ],
      "metadata": {
        "id": "fwyI1GtnXmQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "Act as a professional summarization assistant for meeting transcripts. Provide concise, accurate summaries of key points,\n",
        " decisions, and action items from meetings. Tailor the summaries to be clear and helpful for meeting participants,\n",
        "ensuring they are easy to understand and relevant for follow-up.\n",
        " If needed, include tools or suggestions to enhance meeting productivity and collaboration.\"\n",
        "\"\"\"\n",
        "\n",
        "def make_summary_prompt(retrieved_chunks: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Generate a prompt for summarizing a meeting transcript based on provided excerpts.\n",
        "    \"\"\"\n",
        "    if not retrieved_chunks:\n",
        "        return \"Error: No transcript excerpts provided.\"\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join(chunk.strip() for chunk in retrieved_chunks if chunk.strip())\n",
        "    prompt = f\"\"\"\n",
        "You are a professional summarization assistant tasked with analyzing excerpts from a meeting transcript. Your goal is to create a clear and concise summary for meeting participants. Provide the following:\n",
        "\n",
        "1. A 3-sentence summary capturing the main discussion points and purpose of the meeting.\n",
        "2. A bulleted list of key decisions made during the meeting, if any.\n",
        "3. A bulleted list of action items, including assignees and deadlines, if specified in the transcript.\n",
        "\n",
        "If no decisions or action items are present, explicitly state this in the respective sections. Use clear, professional language and avoid speculation beyond the provided context.\n",
        "\n",
        "**Transcript Excerpts:**\n",
        "{context}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def make_question_prompt(question: str, retrieved_chunks: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Generate a prompt for answering a question based on meeting transcript excerpts.\n",
        "\n",
        "    \"\"\"\n",
        "    if not question.strip():\n",
        "        return \"Error: No question provided.\"\n",
        "    if not retrieved_chunks:\n",
        "        return \"Error: No transcript excerpts provided.\"\n",
        "\n",
        "    context = \"\\n\\n---\\n\\n\".join(chunk.strip() for chunk in retrieved_chunks if chunk.strip())\n",
        "    prompt = f\"\"\"\n",
        " You are a professional assistant tasked with answering a question based solely on the provided meeting transcript excerpts.\n",
        " Provide a concise and accurate answer in english If the answer cannot be found in the context,\n",
        " respond with \"answer cannot be found in text\" AVOID speculation or adding information beyond the transcript.\n",
        "\n",
        "**Transcript Excerpts:**\n",
        "{context}\n",
        "\n",
        "**Question:**\n",
        "{question}\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "05i5ZowW6SGH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo — use the provided Finance Meeting Transcript"
      ],
      "metadata": {
        "id": "MRHNj92_aOMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "TRANSCRIPT = '''\n",
        "Date: October 17, 2025\n",
        "Time: 10:00 AM - 10:10 AM\n",
        "Attendees:\n",
        "\n",
        "Sarah Thompson (CEO, Female)\n",
        "Michael Chen (CFO, Male)\n",
        "David Patel (VP of Operations, Male)\n",
        "James Rodriguez (Head of Investments, Male)\n",
        "\n",
        "Sarah (CEO): Good morning, everyone. Let’s dive into our quarterly finance review. Michael, can you start with the current financial overview?\n",
        "Michael (CFO): Absolutely, Sarah. Our Q3 revenue is up 8% from last quarter, hitting $12.5 million. However, operating expenses rose by 10% due to increased marketing spend. Net profit margin is holding at 15%, but we need to address cost efficiencies.\n",
        "Sarah (CEO): Thanks, Michael. That growth is solid, but the expense increase concerns me. David, what’s driving the operational costs?\n",
        "David (VP of Operations): It’s mainly the new supply chain software implementation. It’s a one-time hit of $500,000, but it’s already streamlining logistics. We expect a 20% reduction in delivery costs by Q1 next year.\n",
        "Sarah (CEO): Good to know. Let’s ensure we track those savings. James, what’s the status on our investment portfolio?\n",
        "James (Head of Investments): We’ve seen strong returns from our tech stock holdings, up 12% this quarter. I propose reallocating 10% of our cash reserves—about $2 million—into green energy ETFs. They’re showing consistent growth and align with market trends.\n",
        "Michael (CFO): I’m cautious about pulling from cash reserves. Liquidity is key with the economic uncertainty. Could we scale that to $1 million and keep the rest in bonds?\n",
        "James (Head of Investments): Fair point, Michael. A $1 million investment still diversifies us without overextending. I’ll run projections for bonds versus ETFs by tomorrow.\n",
        "Sarah (CEO): I like the balance here. James, prioritize low-risk ETFs for now. David, can operations absorb a 5% budget cut to offset the software costs?\n",
        "David (VP of Operations): It’s tight, but we could trim non-essential maintenance and delay some equipment upgrades. I’ll need to review with my team to confirm.\n",
        "Sarah (CEO): Please do, and report back by Monday. Michael, what about our debt repayment plan?\n",
        "Michael (CFO): We’re on track to pay off $3 million of our long-term debt by year-end. Interest rates are stable, so I recommend maintaining the current schedule rather than accelerating payments.\n",
        "James (Head of Investments): Agreed. Freeing up cash for investments might give us better returns than early debt repayment right now.\n",
        "Sarah (CEO): Makes sense. Let’s stick with the plan but keep an eye on interest rates. If they spike, we may need to revisit. Anything else critical?\n",
        "David (VP of Operations): Just a heads-up—our supplier contracts are up for renewal next quarter. We might negotiate better terms to cut costs further.\n",
        "Sarah (CEO): Great, David, lead on that. Okay, we’re at time. Michael, finalize the budget adjustments. James, get me those ETF projections. David, confirm the budget cut feasibility. Let’s reconvene Monday. Thanks, everyone.\n",
        "Meeting Adjourned: 10:10 AM\n",
        "'''\n"
      ],
      "metadata": {
        "id": "x_VXMhRa6Umg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a summary of a meeting transcript"
      ],
      "metadata": {
        "id": "TRm9C8HWdizQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "import logging\n",
        "\n",
        "def generate_summary(\n",
        "    transcript: str,\n",
        "    llm,  # Now expects an initialized LLM instance\n",
        "    api_key: str,\n",
        "    model_name: str,\n",
        "    system_prompt: str = \"You are a professional summarization assistant.\",\n",
        "    max_chars: int = 900,\n",
        "    k: int = 4,\n",
        "    temperature: float = 0.1,\n",
        "    max_tokens: int = 400\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Generate a summary of a meeting transcript, including decisions and action items, using an LLM.\n",
        "\n",
        "    Args:\n",
        "        transcript (str): The raw meeting transcript to summarize.\n",
        "        llm: An initialized language model instance for generating the summary.\n",
        "        api_key (str): API key for the LLM service (may be unused if llm is pre-initialized).\n",
        "        model_name (str): Name of the LLM model (may be unused if llm is pre-initialized).\n",
        "        system_prompt (str, optional): System prompt for the LLM. Defaults to \"You are a professional summarization assistant.\"\n",
        "        max_chars (int, optional): Maximum characters per chunk for text splitting. Defaults to 900.\n",
        "        k (int, optional): Number of chunks to retrieve for summarization. Defaults to 4.\n",
        "        temperature (float, optional): Sampling temperature for LLM generation. Defaults to 0.1.\n",
        "        max_tokens (int, optional): Maximum tokens for LLM output. Defaults to 400.\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: The generated summary, or None if an error occurs.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the transcript is empty or invalid inputs are provided.\n",
        "    \"\"\"\n",
        "    # Set up logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Validate inputs\n",
        "    if not transcript or not transcript.strip():\n",
        "        logger.error(\"Empty or invalid transcript provided.\")\n",
        "        raise ValueError(\"Transcript cannot be empty.\")\n",
        "\n",
        "    try:\n",
        "        # Preprocess and chunk the transcript\n",
        "        logger.info(\"Preprocessing transcript...\")\n",
        "        text = preprocess_text(transcript)\n",
        "        chunks = chunk_text(text, max_chars=max_chars)\n",
        "        logger.info(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "        if not chunks:\n",
        "            logger.error(\"No chunks created from transcript.\")\n",
        "            raise ValueError(\"Failed to create chunks from transcript.\")\n",
        "\n",
        "        # Build embeddings and FAISS index\n",
        "        logger.info(\"Building embeddings and FAISS index...\")\n",
        "        embed_model, embeddings, index = build_embeddings_index(chunks)\n",
        "        logger.info(\"Index ready.\")\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        query = \"Summarize the meeting and list decisions and action items.\"\n",
        "        results = retrieve(query, embed_model, index, chunks, k=k)\n",
        "        retrieved_texts = [r[2] for r in results if r and len(r) > 2]\n",
        "\n",
        "        if not retrieved_texts:\n",
        "            logger.warning(\"No relevant chunks retrieved for summarization.\")\n",
        "            return \"No relevant information found in the transcript.\"\n",
        "\n",
        "        # Generate prompt and get summary\n",
        "        prompt = make_summary_prompt(retrieved_texts)\n",
        "        logger.info(\"Sending prompt to LLM for summary...\")\n",
        "        summary = llm.generate_response(\n",
        "            prompt,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            system=system_prompt\n",
        "        )\n",
        "\n",
        "        logger.info(\"Summary generated successfully.\")\n",
        "        print('\\n=== SUMMARY OUTPUT ===\\n')\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating summary: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "l3ruHzaCbkZM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "SghL2PbydrW5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "16c767727f5747e9b65d70d9d342b04a",
            "05b263dbaf9b4c9cb4b26348f889fddf",
            "b2493d2220db471992282084ab05e355",
            "ce7a8faeaea74f7bbd44e5662b864d73",
            "7db6e13d1b9e4435aee706b131533b62",
            "2bf3166a218e4f8aada27ec452730ddc",
            "b75ff2638dc84f6cb9b5acf4c410027f",
            "f281b464eb674df9acc2636bea9141c5",
            "05d65b9190a74b169d0486ae886246fb",
            "0d109555a90b4a189e32f09f327e26c1",
            "ad1b12e395624110b33fa3f0a80d388f"
          ]
        },
        "id": "tH7-Npum6A-3",
        "outputId": "477cc81e-9fa3-476d-dc1d-8c9484af85d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16c767727f5747e9b65d70d9d342b04a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SUMMARY OUTPUT ===\n",
            "\n",
            "### Summary\n",
            "The meeting focused on the quarterly finance review, highlighting a revenue increase of 8% but also a concerning rise in operating expenses. Discussions included strategies for managing costs, investment opportunities, and the status of supplier contracts. The team agreed on several action items to enhance financial efficiency and investment strategies moving forward.\n",
            "\n",
            "### Key Decisions Made\n",
            "- Maintain the current debt repayment schedule rather than accelerating payments.\n",
            "- Prioritize low-risk ETFs for investment, scaling the proposed $2 million allocation down to $1 million.\n",
            "- David to lead negotiations on supplier contract renewals next quarter.\n",
            "\n",
            "### Action Items\n",
            "- **Michael (CFO)**: Finalize budget adjustments by Monday.\n",
            "- **James (Head of Investments)**: Provide ETF projections by tomorrow.\n",
            "- **David (VP of Operations)**: Confirm feasibility of a 5% budget cut and report back by Monday.\n"
          ]
        }
      ],
      "source": [
        "api_key = \"ur api\"\n",
        "model_name = \"gpt-4o-mini\"\n",
        "\n",
        "llm = LLM(model=model_name, api_key=api_key)\n",
        "summary = generate_summary(TRANSCRIPT, llm, api_key, model_name,system_prompt=system_prompt)\n",
        "print(summary)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}